#introduce Package

install.packages('package-name')
library(package-name)  #package can be found at https://cran.r-project.org

# 1. use the R Console as a calculator. 
2+3
2*3
3*4
8/2
sqrt(2) # square root
exp(2) # natural exponential
2^3     # power
<= # Less than or equal to:
== # Is equal to:
!= # Is NOT equal to:
the AND operator (&)
the OR operator (|)
the NOT operator, otherwise known as the bang operator (!)

# Define and use variables:
a<-1
b<-2
a+b

#2.Vector, matrices and data frame
# Vectors:
x<-c(5,2,6,1,4,1)
y<-c('a','b','c','d','e','f')
z<-1:6
typeof(vector_name) #check the type of elements in a vector
length(vector_name) #check the length of a vector

# Vector combinatuon
v<-c(x,y,z) #x,y,z三个向量合并

# Matrices
A<-matrix(y,2,6) #以向量y为基础，构建两行6列的矩阵

A[1,2] #矩阵A第一行第二列的元素

A[1,2]<-1000 #将a12的数值改为1000

# Data frame
data<-data.frame(x,y,z)      #can include vectors of different type, but need to be same length
#You can think of a data frame as a spreadsheet or as a SQL table.
data[1,2] # element
data[1,] # row (observation)
data[,2] # column (variable)
data$x # x所有元素

#3.Functions in R

functionName(value)

mean(x)
sum(x)
max(x)
sd(x)
table(y) # frequency table
hist(x)
pie(table(y))

help(hist) # 查找某个function的详情可以用help指令

# 4.Data import and export:
getwd() #找到现在work directory的路径
setwd("/Users/zhangkaibang/Desktop/RData") #将路径设置为/Users/zhangkaibang/Desktop/RData
                                           #以后有要倒入的文件可以放到RData里
#for .txt using:  
read.table(), 
#for .csv, 
using read.csv() 

header=TRUE #table里面第一行用于列名称，不进行运算 和read. 搭配使用
header=FALSE #table第一行就进行运算 

#Show Export
head() #将导入的数据显示出来  returns the first 6 rows of a data frame
head(name,n) #first n rows of data frame
attach() #添加路径存储的索引，相当于绑定一个数据框 
detach()解除数据路径存储的绑定
summary() # returns the first 6 rows of a data frame

#If-Else Statement

if (TRUE) {
   print("Go to sleep!")
} else {
   print("Wake up!")
}



dplyr & readr packages

1) pipe operator
df %>%
head()      # equals to head(df)

2) 选中data frame 中的某些列 
select(data frame name, column name1, column name2...) #eg select(customers,age,gender)
or
data frame name %>%                        ##eg customers %>%
select( column name1, column name2...)      ##  select(age,gender)

3）Excluding Columns
df_name %>%
  select(-name1,-name2...)

4)Choosing Rows
data frame %>%
  filter(name1=='xxx',z>SSS)
#eg:
orders %>%
  filter(shoe_material == 'faux-leather',price > 25)
orders %>%
  filter(shoe_type == 'clogs' | price < 20) # 显示鞋子种类为clogs或价格低于20的所有rows
orders %>%
  filter(!(shoe_color == red)) #显示除了鞋子颜色是红色的所有rows

4)Arranging Rows
arrange() #arrange默认数字由小到大，字母从A到Z排列，某则要加desc表示descending order

#eg
customers %>%
  arrange(desc(age))
customers %>%
  arrange(name)
 
5)Adding a Column
mutate()
#eg
df %>%
  mutate(sales_tax = price * 0.075)

6)Adding Multiple Columns
mutate(a,b,c....)
#eg.
df %>%
  mutate(profit = price - cost_to_manufacture,
         in_stock = TRUE)

7)Transmute Columns # only keeping the new columns you add
transmute(a=,b=,...)

8)Rename Columns
rename(new_column_name = old_column_name)

#check renamed column: names() or colnames() #confirm the names of the columns


#
###Data Cleaning

1)  organize data better in dplyr and readr
files <- list.files(pattern = "file_.*csv")  #The first line uses list.files() and a regular expression to find any file in the current directory 
                                              that starts with 'file_' and has an extension of csv, storing the name of each file in a vector files
df_list <- lapply(files,read_csv)            #read each file in files into a data frame with read_csv(), storing the data frames in df_list
df <- bind_rows(df_list)                      #concatenates all of those data frames together with dplyr’s bind_rows() function

2)Counting rows: nrow(df_name)

3)Reshaping your Data
df %>%
  gather('Checking','Savings',key='Account Type',value='Amount')
  
4)count(df,column name):takes a data frame and a column as arguments,returns a table with counts of the unique values in the named column



####Confidence intervals for SD
confint(ModelFull,level = 0.95)

# or derive Lower bound and Upper bounbd respectively
beta[2]-qt(0.975,17)*beta_sd[2] #qt(0.975,17) gives us the 0.975 quantile of a student-t with 17 degrees of freedom.
beta[2]+qt(0.975,17)*beta_sd[2]

####Confidence intervals for  error variance sigma^2
s2<-summary(ModelFull)$sigma^2
# Lower bound
(17*s2)/qchisq(0.975,17)
# Upper bound:
(17*s2)/qchisq(0.025,17) #qchisq() is giving us the qualtiles of a chi-squared distribution.


# 2) Confidence and prediction interval for the response 

# Confidence and prediction intervals for the response variables can be obtained
# using the function predict().
# First note that  predict(ModelFull) only give you the fitted values, same as ModelFull$fitted.values

# However, what you can do is to give new values of the predictor to the function:

New.Values <- data.frame(Air=60,Temp=20,Acid=59)
predict(ModelFull, New.Values)

# Using the parameter interval in the function, you can also produce
# confidence interval:

predict(ModelFull, New.Values, interval="confidence")

# and prediction intervals:

predict(ModelFull, New.Values, interval="prediction")

# The default level is 95%, but you can change it:

predict(ModelFull, New.Values, interval="prediction", level=0.99)




# 3) The ANOVA table  

# To replicate the ANOVA table seen in the lecture we need to force R to compare
# the fitted model only with the so called null model, ie. the model with only the
# intercept:

ModelNull <- lm(Stkloss~1)
anova(ModelNull,ModelFull)


# 4) Coefficients of determinations
# We need to compute it manually:

X<-model.matrix(ModelFull) # to obtain the design matrix X of the model.
H<-X%*%solve(t(X)%*%X)%*%t(X)  # this computes the projection matrix H
h<-diag(H)                    # this extracts the diagonal of the matrix (elements h_ii)
PRESS<-sum((ModelFull$residuals/(1-h))^2) # this computes the Predictive residuals 
SST<-sum((Stkloss-mean(Stkloss))^2)                # this computes explicitly the total sum of squares  

# And finally, the predictive R^2:
Rp<-1-PRESS/SST
Rp


#aic
#The smaller the AIC value, the better the model fit.
The Akaike information criterion (AIC) is a mathematical method for evaluating 
how well a model fits the data it was generated from. 
In statistics, AIC is used to compare different possible models and determine
which one is the best fit for the data.
https://www.scribbr.com/statistics/akaike-information-criterion/#:~:
text=The%20Akaike%20information%20criterion%20(AIC,best%20fit%20for%20the%20data.

#Prected R squred implementation:
pred.R2<-function(model){
  X<-model.matrix(model) # to obtain the design matrix X of the model.
  H<-X%*%solve(t(X)%*%X)%*%t(X)  # this computes the projection matrix H
  h<-diag(H)                    # this extracts the diagonal of the matrix (elements h_ii)
  PRESS<-sum((model$residuals/(1-h))^2) # this computes the Predictive residuals 
  SST<-sum((model$model[,1]-mean(model$model[,1]))^2)                # this computes explicitly the total sum of squares  
  
  # And finally, the predictive R^2:
  Rp<-1-PRESS/SST
  Rp
}

pred.R2(ModelFull)
pred.R2(ModelRed)
pred.R2(Model2)

#Step-wise search:
we can use it to find the best fit model in R:

Model.best<-step(ModelFull)
summary(Model.best)
plot(Model.best) 



##########################################
Practical 5

# Mean effect plots
sup.name<-unique(supp) # define the levels of the factor
mean.effects<-c(mean(len[supp==sup.name[1]]),mean(len[supp==sup.name[2]])) # compute the mean of the response for each level
#values of the mean effect for supplement
mean.effects

# plot of the mean effect for supplement
plot(c(-1,1),mean.effects,xaxt='n',type='b', xlab="Supplement type",ylab="Mean length", main = "Mean effect of supplement type")
axis(2,at=c(15,16,17,18,19,20,21))
axis(1,at=c(-1,1),labels=sup.name)

# same for the dose:
dose.name<-unique(dose)
mean.effects<-c(mean(len[dose==dose.name[1]]),mean(len[dose==dose.name[2]]),mean(len[dose==dose.name[3]]))
mean.effects
plot(dose.name,mean.effects,type='b',xlab="dose",ylab="Mean length",main="Main effect of the dose")


# intercation plot:

interaction.plot(dose,supp,len)

# Discussion point 1:

# simply looking at this visual exploration, how would you describe
# the effect of the dose and the supplement on the tooth length and their interaction?


# Now, fit a linear regression model to confirm this:

# This will treat dose as a continuous variable:

model_ANOVA<-lm(len~dose*supp)
summary(model_ANOVA)

# If the dose should be treated as categorical in you model,
# you need to force it to be factor:

Dose<-factor(dose)

model_TWO_WAY_ANOVA<-lm(len~Dose*supp)
summary(model_TWO_WAY_ANOVA)


# Which approach to be used depend on the practical problem at hand,
# i.e. if there is an interest in a continuous response from the dose or not.


# Warning: it is always good practice to check that the variable tha you want to treat as
# categorical are coded as fator in R:

is.factor(dose)
is.factor(Dose)

# since this is not always immediately obvious looking at columns in a dataset.

# Discussion point 2:

# Interpret the two models that you have fitted. Do they confirm
# your impressions from the exploratory data analysis? Is there any
# difference in interpretation between the two (Hint: look at the interaction terms)?


# 2) Optimisation

# Let us use some simulated data:

set.seed(100) # this is to have the same simulate data, but you can try changing it!
x1<-c(-1,-0.5,0.5,1,-1,-0.5,0.5,1,0.3,-0.3,0.7,-0.7)
x2<-sample(x1,length(x1)) # x2 is a reshuffled version of x1
y<-(x1-0.5)^2+(x2+0.7)^2+x1*x2+rnorm(length(x1),0,0.1)
data<-data.frame(y,x1,x2)
plot(data)

# Goal: find the values of x1 and x2 that minimise y.

# Let's fit a quadratic model:

model_opt<-lm(y~x1*x2+I(x1^2)+I(x2^2)) 
summary(model_opt)

# In this case we know this is the right model, in a
# real application we may need to do remove some of the terms.

# Let's extract the vector b and matrix B needed to compute the stationary point:

beta<-model_opt$coefficients
b<-beta[2:3]
B<-matrix(0,2,2)
B[1,1]<-beta[4]
B[2,2]<-beta[5]
B[1,2]<-B[2,1]<-0.5*beta[6]

x_opt<--0.5*solve(B)%*%b
x_opt

# Now we need to check if it a maximum, a minimum or neither.

eigen(B)$values # eigenvalues of B

# Remember: if they are all positive, it is a minimum
#           if they are all negative, it is a maximum
#           else, it is neither.

# Second thing to check is that it is in the range where we have observations:

range(x1)
range(x2)

# Discussion point 3: Is it within the observation range?

# If it is not, the "correct" answer (apart for collecting more data), it 
# is to choose the closest point in the range of the observations.


# Optional: try to define a new grid x1 and x2 centered on the value you found,
#  simulate new data and repeat the exercise.



# 3) Generalised linear models:

# Poisson regression

# The data in the file insurance.txt (on the KEATS page) show the number of claims during
# the last financial year and the
# ages of 35 customers for a particular health insurance plan.
# The insurance company wants to investigate how the number of claims
# is related to a customer's age.
# Read the data into R:

insdat <- read.table("insurance.txt", header=TRUE)
attach(insdat)

# We can fit a Poisson regression model using the glm command:

i1.glm <- glm(nclaims ~ age, family=poisson(link="log"))

# Let's have a look at the summary:

summary(i1.glm)

# Discussion point 4: 

# how would you describe the relationship between age and the number of insurance claims?

# We can do model selection using the step() function again:

i2.glm<-step(i1.glm)

# In this case the only option would be to remove the age as predictor,
# and the AIC suggests to keep it in the model!

# and we can plot the relationship between age and the expected number of insurance claim:

age_x<-seq(min(age),max(age))
beta<-i2.glm$coefficients
plot(age,nclaims)
points(age_x,exp(beta[1]+beta[2]*age_x),type='l',lwd=2)

# and check the diagnostics plots:
plot(i2.glm)

# Binary (logistic) regression

# Data are collected for 40 patients receiving
# a new surgery technique. The variable surv takes the value 1
# if the patient show any  negative side effect in the 30 days after surgery and is 0 otherwise.
# The age (years) of the patient is also recorded. 
# 
# This are binary data, where the response is 0/1, and 
# we need to use a Bernoulli regression, also called LOGISTIC regression (from the name of the link function).
# The question of interest is to see how the probability $p$ of having negative side effects
#  depends on age.

# Import data and visual exploration:

surg <- read.table("surgery.txt",header=T)
attach(surg)
head(surg)
plot(Age, surv)

# Fit a logistic regression/ Bernoulli regression model:

surg1.glm <- glm(surv ~ Age, binomial, data=surg)
summary(surg1.glm)

# Again, we can use the step() function to see if we should keep Age in the model:

step(surg1.glm)

# What do you think?

# Let's also plot the probability of having side effect 
# (i.e. the mean of our Bernoulli response) given age:

plot(Age, surv, pch=20)
coefs = surg1.glm$coefficients
Ages = seq(from=49, to=73, length.out=1000)
g_mu = coefs[1] + Ages*coefs[2]
probs = exp(g_mu)/(1 + exp(g_mu)) # inverse of the link function
points(Ages, probs, type="l")

# What would you conclude from this graph?







